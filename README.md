# Rakuten Product Classification â€” MLOps (DataScientest)

Projet de pipeline MLOps complet pour la classification des produits Rakuten (texte + image) avec **traÃ§abilitÃ©**, **versioning** et **suivi d'expÃ©riences**.

## ğŸ”§ Stack (progressive)
- **Python 3.11** (venv) â€” âš ï¸ Requis pour Prefect
- **DVC** + **Dagshub** (versioning data/modÃ¨les)
- **MLflow** + **PostgreSQL** + **FastAPI** (Docker) â€” suivi d'expÃ©riences & serving
- **Artifacts MLflow**: S3 (via variables d'environnement)
- **Prefect** (orchestration) â€” installÃ©
- **Ã€ venir** : CI/CD GitHub Actions, Prometheus/Grafana, Evidently

## ğŸ“¦ DonnÃ©es
```
data/
â”œâ”€ raw/
â”‚  â”œâ”€ X_train.csv
â”‚  â”œâ”€ Y_train.csv
â”‚  â”œâ”€ X_test.csv
â”‚  â””â”€ images/image_train/   (fichiers: image_<imageid>*product*<productid>.jpg)
â”œâ”€ interim/
â”‚  â””â”€ merged_train.csv
â””â”€ processed/
â”œâ”€ train_features.csv
â””â”€ predictions.csv

```

## ğŸ—‚ï¸ Arborescence du repo (principal)
```
src/
â”œâ”€ data/make_dataset.py            # ingestion + contrÃ´les
â”œâ”€ features/build_features.py      # prÃ©traitement & features texte
â””â”€ models/
â”œâ”€ train_model.py               # entraÃ®nement + logs MLflow
â””â”€ predict_model.py             # infÃ©rence sur X_test
docker-compose.mlflow.yml          # MLflow + Postgres (Docker)
Dockerfile.mlflow                  # image MLflow custom (psycopg2)
dvc.yaml                           # pipeline DVC (ingestâ†’featuresâ†’trainâ†’predict)

````

## ğŸš€ DÃ©marrage rapide

### 1) Environnement Python
```bash
# Utiliser Python 3.11 (requis pour Prefect)
python3.11 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2) Variables d'environnement (S3 / MLflow)

âš ï¸ **IMPORTANT** : CrÃ©er un fichier `.env` Ã  la racine du projet (non commitÃ©, dÃ©jÃ  dans `.gitignore`) :

```bash
# AWS S3 Configuration for MLflow
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_DEFAULT_REGION=eu-west-1
S3_BUCKET_NAME=your-bucket-name
```

Docker Compose charge automatiquement ce `.env` pour les containers.

### 3) Lancer les services Docker (MLflow + PostgreSQL + API)

```bash
# DÃ©marre tous les services (mlflow, postgres, rakuten_api)
docker-compose -f docker-compose.api.yml up -d

# VÃ©rifier que tout tourne
docker ps

# UI MLflow: http://localhost:5000
# API Rakuten: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

**ArrÃªter tous les containers** :
```bash
docker-compose -f docker-compose.api.yml down
```

**RedÃ©marrer aprÃ¨s modification** :
```bash
docker-compose -f docker-compose.api.yml down
docker-compose -f docker-compose.api.yml up --build -d
```

### 4) ExÃ©cuter le Pipeline (Prefect)

âš ï¸ **CRITIQUE** : Pour que les artifacts MLflow soient sauvegardÃ©s sur S3 (et non localement), **vous DEVEZ charger les variables d'environnement** avant d'exÃ©cuter le pipeline :

```bash
# Activer l'environnement virtuel
source .venv/bin/activate

# âš ï¸ IMPORTANT: Charger les variables AWS depuis .env
export $(cat .env | grep -v '^#' | xargs)

# DÃ©finir l'URI de MLflow
export MLFLOW_TRACKING_URI=http://localhost:5000

# Lancer le pipeline complet (ingest â†’ features â†’ train â†’ predict)
python flows/pipeline_flow.py
```

**Commande complÃ¨te en une ligne** :
```bash
source .venv/bin/activate && export $(cat .env | grep -v '^#' | xargs) && export MLFLOW_TRACKING_URI=http://localhost:5000 && python flows/pipeline_flow.py
```

### 5) Pipeline DVC (alternative reproductible)

```bash
# Charger les variables d'environnement
export $(cat .env | grep -v '^#' | xargs)

# ExÃ©cute ingest â†’ features â†’ train â†’ predict
dvc repro

# Pousse les artefacts (data/modÃ¨les) vers le remote DVC (Dagshub)
dvc push
```

### 6) Scripts unitaires

```bash
# Charger .env d'abord
export $(cat .env | grep -v '^#' | xargs)

# Build features
python src/features/build_features.py

# EntraÃ®nement (log MLflow + artefacts S3)
python src/models/train_model.py

# PrÃ©dictions sur X_test
python src/models/predict_model.py
```

## ğŸ§­ Bonnes pratiques & traÃ§abilitÃ©

* **MLflow** : chaque run logue paramÃ¨tres, mÃ©triques et artefacts (modÃ¨le, vectorizer, metrics.json).
  Des tags lient le run au **commit Git** (`git_commit`, `git_branch`) et la run joint `dvc.yaml`/`dvc.lock`.
* **DVC** : gÃ¨re les outputs de pipeline (`data/interim`, `data/processed`, `models/*`) et synchronise vers Dagshub.
* **Branches** : travail sur `feat/seba/bootstrap` puis PR vers `main`.

## ğŸ†˜ DÃ©pannage rapide

* **Erreur `OSError: [Errno 30] Read-only file system: '/mlflow'`** : 
  - âš ï¸ **Vous avez oubliÃ© de charger les variables d'environnement !**
  - Solution : `export $(cat .env | grep -v '^#' | xargs)` avant d'exÃ©cuter le pipeline
  - Les artifacts MLflow doivent aller sur S3, pas en local

* **S3 auth fail / Access Denied** : 
  - VÃ©rifier que `.env` est bien chargÃ© dans le shell : `echo $AWS_ACCESS_KEY_ID`
  - VÃ©rifier les droits IAM sur le bucket S3
  - VÃ©rifier que `S3_BUCKET_NAME` est bien dÃ©fini

* **MLflow experiment with local artifact path** :
  - L'expÃ©rience a Ã©tÃ© crÃ©Ã©e avant que S3 soit configurÃ©
  - Solution : recrÃ©er l'expÃ©rience ou redÃ©marrer les containers Docker

* **DVC "tracked by SCM"** : 
  - Retirer du suivi Git (`git rm -r --cached <fichier>`) avant de dÃ©clarer en output DVC

* **Images manquantes** : 
  - Le chemin attendu est `data/raw/images/image_train/` avec le motif `image_<imageid>_product_<productid>.jpg`

* **Docker containers not starting** :
  - VÃ©rifier que le fichier `.env` existe Ã  la racine
  - VÃ©rifier les logs : `docker logs sep25_cmlops_rakuten-mlflow-1`

## ğŸ“Œ Licence

Voir `LICENSE`.
MD

```

