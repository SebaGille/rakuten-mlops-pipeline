# Rakuten Product Classification ‚Äî MLOps Pipeline

> ‚ö†Ô∏è **Note on Project Origin**: This repository was initially forked from an **empty school template**. 100% of the visible code, MLOps architecture, and implementation was developed by me personally. The initial fork only served as an empty folder structure.

A complete MLOps pipeline for Rakuten product classification (text + images) with **traceability**, **versioning**, and **experiment tracking**.

## üîß Tech Stack
- **Python 3.11** (venv) ‚Äî ‚ö†Ô∏è Required for Prefect
- **DVC** + **Dagshub** (data/model versioning)
- **MLflow** + **PostgreSQL** + **FastAPI** (Docker) ‚Äî experiment tracking & serving
- **MLflow Artifacts**: AWS S3 (via environment variables)
- **Prefect** (orchestration) ‚Äî installed
- **Prometheus** + **Grafana** + **Evidently** (monitoring & drift detection) ‚Äî ‚úÖ implemented
- **CI/CD**: GitHub Actions ‚Äî ‚úÖ implemented

## üì¶ Data Structure
```
data/
‚îú‚îÄ raw/
‚îÇ  ‚îú‚îÄ X_train.csv
‚îÇ  ‚îú‚îÄ Y_train.csv
‚îÇ  ‚îú‚îÄ X_test.csv
‚îÇ  ‚îî‚îÄ images/image_train/   (files: image_<imageid>_product_<productid>.jpg)
‚îú‚îÄ interim/
‚îÇ  ‚îî‚îÄ merged_train.csv
‚îî‚îÄ processed/
    ‚îú‚îÄ train_features.csv
    ‚îî‚îÄ predictions.csv
```

## üóÇÔ∏è Repository Structure
```
src/
‚îú‚îÄ data/make_dataset.py            # data ingestion + validation
‚îú‚îÄ features/build_features.py      # preprocessing & text features
‚îî‚îÄ models/
   ‚îú‚îÄ train_model.py               # training + MLflow logging
   ‚îî‚îÄ predict_model.py             # inference on X_test
docker-compose.mlflow.yml          # MLflow + Postgres (Docker)
Dockerfile.mlflow                  # custom MLflow image (psycopg2)
dvc.yaml                           # DVC pipeline (ingest‚Üífeatures‚Üítrain‚Üípredict)
```

## üöÄ Quick Start

### 1) Python Environment
```bash
# Use Python 3.11 (required for Prefect)
python3.11 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2) Environment Variables (S3 / MLflow / Grafana / PostgreSQL)

‚ö†Ô∏è **IMPORTANT**: Create a `.env` file at project root (not committed, already in `.gitignore`):

```bash
# AWS S3 Configuration for MLflow
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_DEFAULT_REGION=eu-west-1
S3_BUCKET_NAME=your-bucket-name

# Grafana Configuration (for monitoring dashboard)
GF_SECURITY_ADMIN_USER=admin
GF_SECURITY_ADMIN_PASSWORD=your_secure_password

# PostgreSQL Configuration (for MLflow backend)
POSTGRES_USER=mlflow
POSTGRES_PASSWORD=your_secure_password
POSTGRES_DB=mlflow
```

üí° **Tip**: You can copy `.env.example` to `.env` and fill in your actual values:
```bash
cp .env.example .env
nano .env  # Edit with your values
```

Docker Compose automatically loads this `.env` file for containers.

### 3) Start Docker Services (MLflow + PostgreSQL + API)

```bash
# Start all services (mlflow, postgres, rakuten_api)
docker-compose -f docker-compose.api.yml up -d

# Check that everything is running
docker ps

# MLflow UI: http://localhost:5000
# Rakuten API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

**Stop all containers**:
```bash
docker-compose -f docker-compose.api.yml down
```

**Restart after modifications**:
```bash
docker-compose -f docker-compose.api.yml down
docker-compose -f docker-compose.api.yml up --build -d
```

### 4) Run the Pipeline (Prefect)

‚ö†Ô∏è **CRITICAL**: For MLflow artifacts to be saved on S3 (not locally), **you MUST load environment variables** before running the pipeline:

```bash
# Activate virtual environment
source .venv/bin/activate

# ‚ö†Ô∏è IMPORTANT: Load AWS variables from .env
export $(cat .env | grep -v '^#' | xargs)

# Set MLflow tracking URI
export MLFLOW_TRACKING_URI=http://localhost:5000

# Run complete pipeline (ingest ‚Üí features ‚Üí train ‚Üí predict)
python flows/pipeline_flow.py
```

**One-line command**:
```bash
source .venv/bin/activate && export $(cat .env | grep -v '^#' | xargs) && export MLFLOW_TRACKING_URI=http://localhost:5000 && python flows/pipeline_flow.py
```

### 5) DVC Pipeline (reproducible alternative)

```bash
# Load environment variables
export $(cat .env | grep -v '^#' | xargs)

# Execute ingest ‚Üí features ‚Üí train ‚Üí predict
dvc repro

# Push artifacts (data/models) to DVC remote (Dagshub)
dvc push
```

### 6) Individual Scripts

```bash
# Load .env first
export $(cat .env | grep -v '^#' | xargs)

# Build features
python src/features/build_features.py

# Training (MLflow logging + S3 artifacts)
python src/models/train_model.py

# Predictions on X_test
python src/models/predict_model.py
```

### 7) Monitoring Stack (Prometheus + Grafana + Evidently)

**Start monitoring services**:
```bash
docker-compose -f docker-compose.monitor.yml up -d

# Access dashboards
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000 (use credentials from .env file: GF_SECURITY_ADMIN_USER / GF_SECURITY_ADMIN_PASSWORD)
```

**Generate Evidently drift report**:
```bash
# Make some predictions first to populate inference log
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"designation": "Product name", "description": "Product description"}'

# Generate drift report
python src/monitoring/generate_evidently.py

# View report: reports/evidently/evidently_report.html
```

### 8) Prefect Deployment & Scheduling

```bash
# Activate the project virtual environment and load environment variables
source .venv/bin/activate
export $(cat .env | grep -v '^#' | xargs)

# Register or update Prefect deployments defined in prefect.yaml
prefect deploy

# Start (or restart) the worker that listens to the process work pool
prefect worker start --pool "monitor-process-pool"

# (Optional) Trigger an immediate run without waiting for the cron schedule
prefect deployment run "monitor-and-retrain/monitor-and-retrain-daily"

# Inspect registered deployments
prefect deployment ls
```

Notes:
- The deployment configuration now lives in `prefect.yaml`; the legacy
  `prefect deployment build/apply` workflow is no longer used.
- The worker command is long-running‚Äîrun it in a dedicated terminal
  (or background service) so scheduled runs at `0 9 * * *` can execute.

## üß≠ Best Practices & Traceability

* **MLflow**: Each run logs parameters, metrics, and artifacts (model, vectorizer, metrics.json).
  Tags link the run to the **Git commit** (`git_commit`, `git_branch`) and the run includes `dvc.yaml`/`dvc.lock`.
* **DVC**: Manages pipeline outputs (`data/interim`, `data/processed`, `models/*`) and syncs to Dagshub.
* **Branches**: Work on feature branches (e.g., `Dev`) then PR to `main`.

## üÜò Troubleshooting

* **Error `OSError: [Errno 30] Read-only file system: '/mlflow'`**: 
  - ‚ö†Ô∏è **You forgot to load environment variables!**
  - Solution: `export $(cat .env | grep -v '^#' | xargs)` before running the pipeline
  - MLflow artifacts must go to S3, not local storage

* **S3 auth fail / Access Denied**: 
  - Check that `.env` is loaded in shell: `echo $AWS_ACCESS_KEY_ID`
  - Verify IAM permissions on S3 bucket
  - Verify that `S3_BUCKET_NAME` is defined

* **DVC "tracked by SCM"**: 
  - Remove from Git tracking (`git rm -r --cached <file>`) before declaring as DVC output

* **Missing images**: 
  - Expected path is `data/raw/images/image_train/` with pattern `image_<imageid>_product_<productid>.jpg`

* **Docker containers not starting**:
  - Verify that `.env` file exists at project root
  - Check logs: `docker logs sep25_cmlops_rakuten-mlflow-1`

* **All predictions returning the same class (e.g., class 10)**:
  - ‚úÖ **FIXED**: This was caused by passing a DataFrame to the model instead of raw text
  - The sklearn Pipeline expects a list/array of strings, not a DataFrame
  - Solution: Pass `model.predict([text])` instead of `model.predict(pd.DataFrame({"text": [text]}))`
  - After fix, restart API: `docker-compose -f docker-compose.api.yml up --build -d`

## üë®‚Äçüíª About This Project

**Skills Demonstrated**:
- ‚úÖ Orchestration with Prefect
- ‚úÖ Experiment tracking with MLflow + PostgreSQL
- ‚úÖ Data and model versioning with DVC + Dagshub
- ‚úÖ Model serving API with FastAPI
- ‚úÖ Containerization with Docker & Docker Compose
- ‚úÖ Artifact storage on AWS S3
- ‚úÖ Monitoring with Prometheus + Grafana (metrics, dashboards)
- ‚úÖ Data drift detection with Evidently
- ‚úÖ CI/CD with GitHub Actions (tests, deployment)
- ‚úÖ Git best practices (branches, tags, atomic commits)
- ‚úÖ Debugging & troubleshooting production issues

**Author**: S√©bastien

## üìå License

See `LICENSE`.

